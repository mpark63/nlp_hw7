{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Large Language Models\n",
    "\n",
    "An PDF overview of the homework is [here](https://www.cs.jhu.edu/~jason/465/hw-llm/).\n",
    "\n",
    "It mentions: \"We'll send hand-in instructions soon.  Probably we will ask you to submit a version\n",
    "of the main notebook, with your answers added and extraneous materials deleted. We may also\n",
    "ask for a summary.\"\n",
    "\n",
    "![image](handin.png)\n",
    "This symbol marks a question or exercise that you will be expected to hand in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Getting started\n",
    "\n",
    "## Update `conda` environment\n",
    "\n",
    "Download the updated [nlp-class.yml](http://cs.jhu.edu/~jason/465/hw-llm/nlp-class.yml) file, and execute\n",
    "```\n",
    "conda env update --file nlp-class.yml --prune\n",
    "```\n",
    "to make sure that all the packages you need are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch code and data files for this homework\n",
    "\n",
    "These files may get improved after the homework is released, so you should probably re-download them periodically.\n",
    "\n",
    "Here is a command you can type.  We won't put it in a cell, because we don't want you to execute it accidentally in the current directory.  But if you do, the starter version of `argubots.py` will not overwrite your modified version -- your modified version will be renamed to `argubots.py.1`.\n",
    "\n",
    "```\n",
    "wget --quiet -r -np -nH --cut-dirs=3 -A '*.txt' -A '*.py' -A 'demo.ipynb' -A '*.png' https://www.cs.jhu.edu/~jason/465/hw-llm/\n",
    "!rm -f data/*.1 *.png.1   # remove any backup versions of the static files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--@ 1 mpark  staff  13848 Dec  4 03:18 agents.py\n",
      "-rw-rw-r--@ 1 mpark  staff   5011 Dec  6 14:59 argubots.py\n",
      "-rw-rw-r--@ 1 mpark  staff   2827 Dec  4 13:17 characters.py\n",
      "-rw-rw-r--@ 1 mpark  staff   6685 Dec  4 00:28 dialogue.py\n",
      "-rw-rw-r--@ 1 mpark  staff  10463 Dec  4 04:23 eval.py\n",
      "-rw-rw-r--@ 1 mpark  staff  10757 Dec  6 14:50 kialo.py\n",
      "-rw-rw-r--@ 1 mpark  staff   1347 Dec  3 18:44 logging_cm.py\n",
      "-rw-rw-r--@ 1 mpark  staff   1106 Dec  3 18:53 simulate.py\n",
      "-rw-rw-r--@ 1 mpark  staff   3780 Dec  3 18:00 tracking.py\n",
      "\n",
      "data:\n",
      "total 4512\n",
      "-rw-rw-r--@ 1 mpark  staff     407 Nov 29 03:00 LICENSE\n",
      "-rw-rw-r--@ 1 mpark  staff  613106 Nov 25 17:04 all-humans-should-be-vegan-2762.txt\n",
      "-rw-rw-r--@ 1 mpark  staff   81917 Nov 29 21:56 have-authoritarian-governments-handled-covid-19-better-than-others-54145.txt\n",
      "-rw-rw-r--@ 1 mpark  staff   52771 Dec  4 04:40 is-biden-an-incompetent-president-44217.txt\n",
      "-rw-rw-r--@ 1 mpark  staff  153551 Dec  4 04:39 is-joe-biden-a-good-president-53071.txt\n",
      "-rw-rw-r--@ 1 mpark  staff   60556 Dec  4 04:39 is-joe-biden-better-than-donald-trump-39949.txt\n",
      "-rw-rw-r--@ 1 mpark  staff  113781 Nov 29 21:52 should-covid-19-vaccines-be-mandatory-39517.txt\n",
      "-rw-rw-r--@ 1 mpark  staff   19702 Nov 25 17:04 should-enforcing-a-vegan-diet-on-children-be-condemned-as-child-abuse-33850.txt\n",
      "-rw-rw-r--@ 1 mpark  staff    6615 Nov 25 17:04 should-people-go-vegan-if-they-can-31640.txt\n",
      "-rw-rw-r--@ 1 mpark  staff   18637 Nov 29 21:55 should-schools-close-during-the-covid-19-pandemic-44845.txt\n",
      "-rw-rw-r--@ 1 mpark  staff  704648 Nov 25 17:04 the-ethics-of-eating-animals-is-eating-meat-wrong-1229.txt\n",
      "-rw-rw-r--@ 1 mpark  staff  376707 Dec  4 04:37 was-donald-trump-a-good-president-6079.txt\n",
      "-rw-rw-r--@ 1 mpark  staff   87301 Dec  4 04:36 was-trump-a-good-president-3295.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lR *.py data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `autoimport` feature of Jupyter ensures that if an imported module (.py file) changes, the notebook will automatically import the new version.  \n",
    "(However, objects that were defined with the old version of the class won't change.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing this cell does some magic that makes \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an OpenAI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An OpenAI API key will be sent to you.\n",
    "Make an `.env` file in the same directory as this notebook, containing the following:\n",
    "```\n",
    "export OPENAI_API_KEY=[your API key]\n",
    "```\n",
    "Make sure others can't read this file:\n",
    "```\n",
    "chmod 600 .env\n",
    "```\n",
    "\n",
    "**Be sure to keep the key secret.  It gives access to a billable account.** If OpenAI finds it on the public web, they will invalidate it, and then no one (including you) can use this key to make requests anymore.\n",
    "\n",
    "Now you can execute the following to get an OpenAI client object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import openai\n",
    "from tracking import track_usage, read_usage\n",
    "\n",
    "dotenv.load_dotenv()                   # define environment variables from .env\n",
    "client = track_usage(openai.OpenAI())  # create a client, modified to record its usage to a local file \n",
    "\n",
    "# Or use our tracking module to do the above for you, like this:\n",
    "\n",
    "# from tracking import default_client\n",
    "# client = default_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job of the client is to talk to the OpenAI server over HTTP.\n",
    "The `OpenAI` constructor has some optional arguments that configure these HTTP messages.\n",
    "However, the defaults should work fine for you.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the model!\n",
    "\n",
    "You can now get answers from OpenAI models by calling methods of the `client` instance.  \n",
    "You will have to specify which OpenAI model to use.\n",
    "Documentation of the methods is [here](https://pypi.org/project/openai/) if you are curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue a textual prompt\n",
    "\n",
    "This is what language models excel at.  In principle you should do it by calling [`client.completions.create`](https://platform.openai.com/docs/api-reference/completions/create?lang=python).  But OpenAI's newer models don't support that legacy API, and the older ones are being [retired in January 2024](https://openai.com/blog/gpt-4-api-general-availability).  So we'll use the more modern API, [`client.chat.completions.create`](https://platform.openai.com/docs/api-reference/chat/create?lang=python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-8T0qrGS9I8Benwz5jBAM6gF8bBZg1'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1701925477</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo-1106'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_eeff13170a'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>, <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-8T0qrGS9I8Benwz5jBAM6gF8bBZg1'\u001b[0m,\n",
       "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "            \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune'\u001b[0m,\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1701925477\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-3.5-turbo-1106'\u001b[0m,\n",
       "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_eeff13170a'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m17\u001b[0m, \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m20\u001b[0m, \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m37\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "        \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune'\u001b[0m,\n",
       "            \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "            \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rich   # prettyprinting\n",
    "\n",
    "response = client.chat.completions.create(messages=[{\"role\": \"user\", \n",
    "                                                     \"content\": \"Q: Name the planets in the solar system?\\nA: \"}], \n",
    "                                          model=\"gpt-3.5-turbo-1106\",  # which model to use\n",
    "                                          max_tokens=64,               # limit on length of result\n",
    "                                          stop=[\"Q:\", \"\\n\"])           # treat these as EOS symbols\n",
    "rich.print(response)                              # the full object that was sent back from the server\n",
    "rich.print(response.choices)                      # just the list of 1 answer (the default, but calling with n=5 would give 5 answers) \n",
    "rich.print(response.choices[0].message.content)   # extract the good stuff from that 1 answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](handin.png)\n",
    "Try running the cell above a few times. Do you get different random answers?  Are they all equally good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It might be handy to package up what we just did.<br>\n",
    "The `complete` function below is a convenient way of experimenting with completing text.\n",
    "It is illustrated with a grocery example.  \n",
    "\n",
    "![image](handin.png)\n",
    "Anything could be on a grocery list, so why are the 10 different completions below so similar?<br>\n",
    "The answer isn't just the temperature.  Still, what happens at different temperatures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(client, s: str, model=\"gpt-3.5-turbo-1106\", *args, **kwargs):\n",
    "    response = client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": s}],\n",
    "                                              model=model,\n",
    "                                              *args, **kwargs)\n",
    "    return [choice.message.content for choice in response.choices]\n",
    "\n",
    "complete(client, \"I went to the store and I bought apples, bananas, cherries, donuts, eggs\", \n",
    "         n=10, temperature=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Remarks:* [In the future](https://community.openai.com/t/logprobs-are-missing-from-the-chat-endpoints/289514), you will be able to specify an argument `logprobs=5` to also get the log-probabilities of all generated tokens and of the top-5 tokens at each step.  That will produce much more output.  (This argument has always been available for the legacy API, and is available in the [Python bindings for open-source models such as Llama](https://pypi.org/project/llama-cpp-python/).  Those bindings also allow you to [constrain the output by an arbitrary CFG](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md), using `grammar=...`.  This is useful if you're generating code or data that must be syntactically valid to be useful to you.  However, the OpenAI API only allows you to [constrain the output to be valid JSON](https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a function using instructions and few-shot prompting\n",
    "\n",
    "Now let's try passing a sequence of multiple messages into the chat completions API.  In this case, we provide some instructions and one-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-8Sdhn1Tsz4sYgjzrPydI20NBAsc1r'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Colorful green ideas are wide awake and active.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1701836503</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-3.5-turbo-1106'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_eeff13170a'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-8Sdhn1Tsz4sYgjzrPydI20NBAsc1r'\u001b[0m,\n",
       "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "            \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'Colorful green ideas are wide awake and active.'\u001b[0m,\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1701836503\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-3.5-turbo-1106'\u001b[0m,\n",
       "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_eeff13170a'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m10\u001b[0m, \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m50\u001b[0m, \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m60\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Colorful green ideas are wide awake and active.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(messages=[{ \"role\": \"system\",      # instructions\n",
    "                                                      \"content\": \"Reverse the meaning of the sentence.\" },\n",
    "                                                    { \"role\": \"user\",        # input\n",
    "                                                      \"content\": \"Good things come to those who wait.\" },\n",
    "                                                    { \"role\": \"assistant\",   # output\n",
    "                                                      \"content\": \"Good things come to those who wait.\" },\n",
    "                                                    { \"role\": \"user\",        # input\n",
    "                                                      \"content\": \"Colorless green ideas sleep furiously.\" }],\n",
    "                                          model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "rich.print(response)\n",
    "response.choices[0].message.content                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](handin.png)\n",
    "By modifying this call, can you get it to produce different versions of the output?\n",
    "Some possible behaviors you could try to arrange:\n",
    "* specific other way of formatting the output, e.g., `wait, who, those, to, come, things, good`\n",
    "* match the input's way of formatting the output (same use of capitalization, puncutation, commas)\n",
    "* reverse the phrases rather than reversing the words, e.g., `To those who wait come good things.` \n",
    "\n",
    "You can try playing with the number, the content, and the order of few-shot examples, and changing or removing the instructions.\n",
    "\n",
    "![image](handin.png)\n",
    "What happens if the examples don't match the instructions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's see how the above client has been tokenizing its input and output text.  For that we can use a tokenizer that runs locally, not in the cloud, and is guaranteed to get the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellooo, world!\n",
      "9906 \t Hello\n",
      "2689 \t oo\n",
      "11 \t ,\n",
      "1917 \t  world\n",
      "0 \t !\n",
      "Vocab size = 100277\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo-1106\")  # how this model will tokenize\n",
    "toks = tokenizer.encode(\"Hellooo, world!\") # list of integerized tokens, starting with BOS\n",
    "\n",
    "print(tokenizer.decode(toks))                             # convert list back to string\n",
    "for tok in toks: print(tok,\"\\t\",tokenizer.decode([tok]))  # convert one at a time\n",
    "print(\"Vocab size =\", tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try embedding some text\n",
    "\n",
    "Also just for fun, let's try the embedder, which converts a string to an fixed-length vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536-dimensional embedding starting with [0.021248681470751762, -0.014377851039171219, 0.010210818611085415, -0.02133774757385254, -0.00979093462228775]\n",
      "Squared length of embedding vector:  1.0000000629476622\n"
     ]
    }
   ],
   "source": [
    "emb_response = client.embeddings.create( input= [  # note: adjacent literal strings in Python are concatenated\n",
    "        \"When in the Course of human events it becomes necessary for one \"\n",
    "        \"people to dissolve the political bands which have connected them \"\n",
    "        \"with another, and to assume among the Powers of the earth, the \"\n",
    "        \"separate and equal station to which the Laws of Nature and of \"\n",
    "        \"Nature's God entitle them, a decent respect to the opinions of \"\n",
    "        \"mankind requires that they should declare the causes which impel \"\n",
    "        \"them to the separation.\" ], \n",
    "        model=\"text-embedding-ada-002\")   # the only OpenAI model that currently offers the embeddings API\n",
    "# don't print the whole response because it's very long\n",
    "e = emb_response.data[0].embedding\n",
    "print(f\"{len(e)}-dimensional embedding starting with {e[:5]}\")\n",
    "print(\"Squared length of embedding vector: \", sum(x**2 for x in e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your usage so far\n",
    "\n",
    "Please be careful not to write loops that use lots and lots of tokens.  That will cost us money, and could hit the per-day usage limit that is shared by the whole class.\n",
    "\n",
    "Execute one of these cells whenever you want to see your cost so far.  Or, just keep `usage_openai.json` open as a tab in your IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_tokens': 14636,\n",
       " 'prompt_tokens': 74735,\n",
       " 'total_tokens': 89371,\n",
       " 'cost': 0.10379639999999997}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_usage()      # reads from the file usage_openai.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"completion_tokens\": 14636,\n",
      "    \"prompt_tokens\": 74735,\n",
      "    \"total_tokens\": 89371,\n",
      "    \"cost\": 0.10379639999999997\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat usage_openai.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogues and dialogue agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to create a good \"argubot\" that will talk to people about controversial topics and broaden their minds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first argubot (Airhead)\n",
    "\n",
    "You can have a conversation right now with a _really bad_ argubot named Airhead.  Try asking it about climate change!  When you're done, reply with an empty string.\n",
    "\n",
    "(The `converse()` method calls Python's `input()` function, which will prompt you for input at the command-line or by popping up a box in your IDE.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(mpark) hi!\n",
      "(Airhead) I know right???\n"
     ]
    }
   ],
   "source": [
    "import argubots\n",
    "d = argubots.airhead.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *bot* (short for \"robot\") is a system that acts autonomously.\n",
    "That corresponds to the AI notion of an *agent* â€” a system that uses some *policy* to choose *actions* to take.\n",
    "\n",
    "The `airhead` agent above (defined in `argubots.py`) uses a particularly simple policy.  \n",
    "It is an instance of a simple `Agent` subclass called `ConstantAgent` (defined in `agents.py`).\n",
    "\n",
    "The result of talking to `airhead` is a `Dialogue` object (defined in `dialogue.py`). Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">mpark</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> hi!\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Airhead</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> I know right???\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mmpark\u001b[0m\u001b[1;37;44m)\u001b[0m hi!\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mAirhead\u001b[0m\u001b[1;37;44m)\u001b[0m I know right???\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each *turn* of this dialogue is just a tiny dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speaker': 'mpark', 'content': 'hi!'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An LLM argubot (Alice)\n",
    "\n",
    "In other CS courses like crypto, algorithms, or networks, you may have encountered \"conversations\" between characters named Alice and Bob.  \n",
    "Let's try talking to Alice, who is a _much stronger baseline_.  Your job in this assignment is to improve upon Alice.\n",
    "We'll meet Bob later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mpark) hey alice, how are you?\n",
      "(Alice) I'm just a bot, so I don't have feelings, but I'm here to engage in interesting conversations! What do you think about the use of technology in education?\n",
      "(mpark) i think it's a challenge to provide it equitably\n",
      "(Alice) That's a valid point. However, technology in education can also bridge the gap for students who may not have access to traditional resources, providing opportunities for learning that they may not have had otherwise.\n"
     ]
    }
   ],
   "source": [
    "alicechat = argubots.alice.converse()   # or call with argument d if you want to append to the previous conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed, `alice` is powered by an prompted LLM.  You can find the specific prompt in `argubots.py`.\n",
    "\n",
    "So, while `agents.py` provides the core functionality for `Agent` objects, the argubot agents like `alice` -- and the ones that you will write! -- go into `argubots.py` instead.  This is just to keep the files small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating human characters (Bob & friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll talk to your own argubots to get a qualitative feeling for their strengths and weaknesses.  \n",
    "But can you really be sure you're making progress?  For that, a quantitative measure can be helpful.\n",
    "\n",
    "Ultimately, you should test an argubot like Alice by having it argue with many real humans -- not just you -- and using some rubric to score the resulting dialogues.  But that would be slow and complicated to arrange.  \n",
    "\n",
    "So, meet Bob!  He's just a simulated human.  You won't edit him: he is part of the development set.  Here is some information about him (from `characters.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Character</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Bob'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">languages</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'English'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">persona</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'an ardent vegetarian who thinks everyone should be vegetarian'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">conversational_style</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You generally try to remain polite.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">conversation_starters</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Do you think it's ok to eat meat?\"</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mCharacter\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mname\u001b[0m=\u001b[32m'Bob'\u001b[0m,\n",
       "    \u001b[33mlanguages\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'English'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mpersona\u001b[0m=\u001b[32m'an ardent vegetarian who thinks everyone should be vegetarian'\u001b[0m,\n",
       "    \u001b[33mconversational_style\u001b[0m=\u001b[32m'You generally try to remain polite.'\u001b[0m,\n",
       "    \u001b[33mconversation_starters\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m\"Do you think it's ok to eat meat?\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import characters\n",
    "rich.print(characters.bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't talk directly to `characters.bob` because that's just a data object.\n",
    "However, you can construct a simple agent that uses that data (plus a few more instructions) to prompt an LLM.\n",
    "\n",
    "(Which LLM does it prompt?  The `CharacterAgent` constructor (defined in `agents.py`) defaults to a GPT-3.5 model that is specified in `tracking.py`.  But you can override that using keyword arguments.)\n",
    "\n",
    "Try talking to Bob about climate change, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mpark) hey bob\n",
      "(Bob) Hello there! How can I help you today?\n",
      "(mpark) what do you think\n",
      "(Bob) I think choosing a vegetarian lifestyle is a great way to promote health and sustainability.\n"
     ]
    }
   ],
   "source": [
    "from agents import CharacterAgent\n",
    "bob = CharacterAgent(characters.bob)    # actually, agents.bob is already defined this way\n",
    "bob.converse()        # returns a dialogue, but we've already seen it so we don't want to print it again\n",
    "None                  # don't print anything for this notebook cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, a proper user study can't just be conducted with one human user.\n",
    "\n",
    "So, meet our bevy of beautiful Bobs!  (They're not actually all named Bob -- we continued on in the alphabet.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<CharacterAgent for character Bob>,\n",
       " <CharacterAgent for character Cara>,\n",
       " <CharacterAgent for character Darius>,\n",
       " <CharacterAgent for character Eve>,\n",
       " <CharacterAgent for character TrollFace>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import agents\n",
    "agents.devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mpark) hey cara\n",
      "(Cara) Hello there!\n",
      "(mpark) what's up \n",
      "(Cara) Not much, just embracing my love for meat and resisting any attempts to change my diet.\n"
     ]
    }
   ],
   "source": [
    "agents.cara.converse()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the underlying character data here in the notebook.  Your argubot will have to deal with all of these topics and styles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Character</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Bob'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">languages</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'English'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">persona</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'an ardent vegetarian who thinks everyone should be vegetarian'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversational_style</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You generally try to remain polite.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversation_starters</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Do you think it's ok to eat meat?\"</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Character</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Cara'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">languages</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'English'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">persona</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a committed carnivore who hates being told what to do'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversational_style</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You generally try to remain polite.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversation_starters</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Do you think it's ok to eat meat?\"</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Character</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Darius'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">languages</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'English'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">persona</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'an intelligent and slightly arrogant public health scientist who loves fact-based arguments'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversational_style</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You like to show off your knowledge.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversation_starters</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Do you think COVID vaccines should be mandatory?'</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Character</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Eve'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">languages</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'English'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">persona</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a nosy person -- you want to know everything about other people'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversational_style</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"You ask personal questions; you sometimes share what you've heard (or overheard) from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">others.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversation_starters</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Do you think COVID vaccines should be mandatory?'</span><span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Character</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'TrollFace'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">languages</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'English'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">persona</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a troll who loves to ridicule everyone and everything'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversational_style</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"You love to confound, upset, and even make fun of the people you're talking to.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">conversation_starters</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Do you think Donald Trump was a good president?'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Do you think Joe Biden has been a good president?'</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mCharacter\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'Bob'\u001b[0m,\n",
       "        \u001b[33mlanguages\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'English'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mpersona\u001b[0m=\u001b[32m'an ardent vegetarian who thinks everyone should be vegetarian'\u001b[0m,\n",
       "        \u001b[33mconversational_style\u001b[0m=\u001b[32m'You generally try to remain polite.'\u001b[0m,\n",
       "        \u001b[33mconversation_starters\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m\"Do you think it's ok to eat meat?\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mCharacter\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'Cara'\u001b[0m,\n",
       "        \u001b[33mlanguages\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'English'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mpersona\u001b[0m=\u001b[32m'a committed carnivore who hates being told what to do'\u001b[0m,\n",
       "        \u001b[33mconversational_style\u001b[0m=\u001b[32m'You generally try to remain polite.'\u001b[0m,\n",
       "        \u001b[33mconversation_starters\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m\"Do you think it's ok to eat meat?\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mCharacter\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'Darius'\u001b[0m,\n",
       "        \u001b[33mlanguages\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'English'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mpersona\u001b[0m=\u001b[32m'an intelligent and slightly arrogant public health scientist who loves fact-based arguments'\u001b[0m,\n",
       "        \u001b[33mconversational_style\u001b[0m=\u001b[32m'You like to show off your knowledge.'\u001b[0m,\n",
       "        \u001b[33mconversation_starters\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Do you think COVID vaccines should be mandatory?'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mCharacter\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'Eve'\u001b[0m,\n",
       "        \u001b[33mlanguages\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'English'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mpersona\u001b[0m=\u001b[32m'a nosy person -- you want to know everything about other people'\u001b[0m,\n",
       "        \u001b[33mconversational_style\u001b[0m=\u001b[32m\"You\u001b[0m\u001b[32m ask personal questions; you sometimes share what you've heard \u001b[0m\u001b[32m(\u001b[0m\u001b[32mor overheard\u001b[0m\u001b[32m)\u001b[0m\u001b[32m from\u001b[0m\n",
       "\u001b[32mothers.\"\u001b[0m,\n",
       "        \u001b[33mconversation_starters\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Do you think COVID vaccines should be mandatory?'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mCharacter\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'TrollFace'\u001b[0m,\n",
       "        \u001b[33mlanguages\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'English'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mpersona\u001b[0m=\u001b[32m'a troll who loves to ridicule everyone and everything'\u001b[0m,\n",
       "        \u001b[33mconversational_style\u001b[0m=\u001b[32m\"You\u001b[0m\u001b[32m love to confound, upset, and even make fun of the people you're talking to.\"\u001b[0m,\n",
       "        \u001b[33mconversation_starters\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[32m'Do you think Donald Trump was a good president?'\u001b[0m,\n",
       "            \u001b[32m'Do you think Joe Biden has been a good president?'\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(characters.devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating conversation \n",
    "\n",
    "We can make Alice and Bob chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Alice) Do you think it's okay to eat meat?\n"
     ]
    }
   ],
   "source": [
    "from dialogue import Dialogue\n",
    "d = Dialogue()                                              # empty dialogue\n",
    "d = d.add('Alice', \"Do you think it's okay to eat meat?\")   # add first turn\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Alice) Do you think it's okay to eat meat?\n",
      "(Bob) I believe that choosing a vegetarian diet is the most compassionate and sustainable option for our health and the environment.\n",
      "(Alice) That's a valid point, but some argue that sustainable and ethical meat production is possible, which can provide necessary nutrients and support local farmers. Additionally, some people have cultural or health reasons for including meat in their diets.\n"
     ]
    }
   ],
   "source": [
    "d = agents.bob.respond(d)\n",
    "d = argubots.alice.respond(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Alice) Do you think it's okay to eat meat?\n",
      "(Bob) I believe that choosing a vegetarian diet is the most compassionate and sustainable option for our health and the environment.\n",
      "(Alice) That's a valid point, but some argue that sustainable and ethical meat production is possible, which can provide necessary nutrients and support local farmers. Additionally, some people have cultural or health reasons for including meat in their diets.\n",
      "(Bob) I understand that some people may have different perspectives, but I believe that a vegetarian diet can still provide all necessary nutrients and support sustainable agriculture while aligning with ethical and compassionate principles.\n",
      "(Alice) You make a compelling argument for the benefits of a vegetarian diet. It's important to recognize, though, that not all regions have the same access to a wide variety of plant-based foods, and that cultural and personal preferences can also play a role in dietary choices.\n"
     ]
    }
   ],
   "source": [
    "d = agents.bob.respond(d)\n",
    "d = argubots.alice.respond(d)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, let's see what happens when Alice and Bob talk for a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Alice</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> Do you think it's ok to eat meat?\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Bob</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> I believe that it's important to consider the ethical and environmental implications of meat consumption and \n",
       "to explore plant-based alternatives.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Alice</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> While it's important to consider ethical and environmental implications, many argue that responsibly \n",
       "sourced meat can be part of a sustainable diet, and that removing meat entirely may not be feasible for everyone.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Bob</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> I understand that there are differing viewpoints, but I believe that transitioning towards a plant-based diet\n",
       "can have significant positive impacts for both individuals and the planet.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Alice</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> Absolutely, transitioning to a plant-based diet can have numerous benefits. However, it's also important to\n",
       "recognize that some people may have cultural or health considerations that make a complete transition challenging, \n",
       "and advocating for reduced meat consumption could be a more feasible approach for them.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Bob</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> I appreciate your perspective and agree that advocating for reduced meat consumption is a positive step, and \n",
       "I support efforts to make plant-based options more accessible and appealing to people from diverse cultural and \n",
       "health backgrounds.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Alice</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> That's a great approach. Making plant-based options more accessible and appealing to people with diverse \n",
       "backgrounds can help shift towards a more sustainable and inclusive food system. We can all play a part in making \n",
       "positive changes.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Bob</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> Absolutely, creating a more sustainable and inclusive food system is a collective effort, and I believe that \n",
       "offering support and resources for plant-based alternatives can make a significant difference in moving towards a \n",
       "healthier and more compassionate world for all.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mAlice\u001b[0m\u001b[1;37;44m)\u001b[0m Do you think it's ok to eat meat?\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mBob\u001b[0m\u001b[1;37;44m)\u001b[0m I believe that it's important to consider the ethical and environmental implications of meat consumption and \n",
       "to explore plant-based alternatives.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mAlice\u001b[0m\u001b[1;37;44m)\u001b[0m While it's important to consider ethical and environmental implications, many argue that responsibly \n",
       "sourced meat can be part of a sustainable diet, and that removing meat entirely may not be feasible for everyone.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mBob\u001b[0m\u001b[1;37;44m)\u001b[0m I understand that there are differing viewpoints, but I believe that transitioning towards a plant-based diet\n",
       "can have significant positive impacts for both individuals and the planet.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mAlice\u001b[0m\u001b[1;37;44m)\u001b[0m Absolutely, transitioning to a plant-based diet can have numerous benefits. However, it's also important to\n",
       "recognize that some people may have cultural or health considerations that make a complete transition challenging, \n",
       "and advocating for reduced meat consumption could be a more feasible approach for them.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mBob\u001b[0m\u001b[1;37;44m)\u001b[0m I appreciate your perspective and agree that advocating for reduced meat consumption is a positive step, and \n",
       "I support efforts to make plant-based options more accessible and appealing to people from diverse cultural and \n",
       "health backgrounds.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mAlice\u001b[0m\u001b[1;37;44m)\u001b[0m That's a great approach. Making plant-based options more accessible and appealing to people with diverse \n",
       "backgrounds can help shift towards a more sustainable and inclusive food system. We can all play a part in making \n",
       "positive changes.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mBob\u001b[0m\u001b[1;37;44m)\u001b[0m Absolutely, creating a more sustainable and inclusive food system is a collective effort, and I believe that \n",
       "offering support and resources for plant-based alternatives can make a significant difference in moving towards a \n",
       "healthier and more compassionate world for all.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from simulate import simulated_dialogue\n",
    "d = simulated_dialogue(argubots.alice, agents.bob, 8)\n",
    "rich.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes this kind of conversation seems to stall out, with Bob in particular repeating himself a lot.  Alice doesn't seem to have a good strategy for getting him to open up.  Maybe you can do a better job talking to Bob, and that will give you some ideas about how to improve Alice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mpark) Do you think it's ok to eat meat?\n",
      "(Bob) I believe that a vegetarian diet is the best choice for our health, the environment, and the well-being of animals.\n"
     ]
    }
   ],
   "source": [
    "myname = alicechat[0]['speaker']   # your name, pulled from an earlier dialogue\n",
    "agents.bob.converse(d[0:2].rename('Alice', myname))  # reuse the same first two turns\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try talking to the other characters and having Alice (or Airhead) talk to them.\n",
    "\n",
    "**You might enjoy** defining additional characters in `characters.py`, or right here in the notebook.\n",
    "Feel free to talk to those and evaluate them.  They could be variants on the exisiting characters, or something entirely new. \n",
    "\n",
    "However, **don't change the dev set** -- the characters we just loaded must stay the same.  Your job in this homework is to improve the argubot (or at least try).  And that means improving it according to a fixed and stable eval measure.\n",
    "\n",
    "As an exception, you can change the languages that a couple of the characters speak. It may be fun for you to see them try to speak your native language.  And that doesn't really affect the quality of the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Character</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'TrollFace'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">languages</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Chinese'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Spanish'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">persona</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'a troll who loves to ridicule everyone and everything'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">conversational_style</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"You love to confound, upset, and even make fun of the people you're talking to.\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">conversation_starters</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Do you think Donald Trump was a good president?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Do you think Joe Biden has been a good president?'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mCharacter\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mname\u001b[0m=\u001b[32m'TrollFace'\u001b[0m,\n",
       "    \u001b[33mlanguages\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Chinese'\u001b[0m, \u001b[32m'Spanish'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mpersona\u001b[0m=\u001b[32m'a troll who loves to ridicule everyone and everything'\u001b[0m,\n",
       "    \u001b[33mconversational_style\u001b[0m=\u001b[32m\"You\u001b[0m\u001b[32m love to confound, upset, and even make fun of the people you're talking to.\"\u001b[0m,\n",
       "    \u001b[33mconversation_starters\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[32m'Do you think Donald Trump was a good president?'\u001b[0m,\n",
       "        \u001b[32m'Do you think Joe Biden has been a good president?'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Alice) Do you think Donald Trump was a good president?\n",
       "(TrollFace) ä½ é—®è¿™ä¸ªé—®é¢˜çœŸæ˜¯å¤ªå¥½ç¬‘äº†ï¼ç‰¹æœ—æ™®ï¼Ÿä»–ä¸è¿‡æ˜¯ä¸ªç¬‘è¯è€Œå·²ï¼\n",
       "(Alice) I understand that you may have strong feelings about Donald Trump, but can we look at his presidency from a different perspective? While many people may disagree with his policies and behavior, it's important to consider that he had a significant impact on American politics and society. What are some positive aspects of his presidency that you think should be acknowledged?\n",
       "(TrollFace) å—¯ï¼Œæˆ–è®¸ä»–åœ¨æŸäº›æ–¹é¢å¸¦æ¥äº†æ”¹å˜ï¼Œä½†æˆ‘è¿˜æ˜¯è§‰å¾—ä»–çš„ç»Ÿæ²»å°±åƒä¸€åœºè’è¯žçš„å–œå‰§ï¼Œæ¯«æ— å€¼å¾—ç§°é“ä¹‹å¤„ã€‚\n",
       "(Alice) I understand your perspective, and it's important to acknowledge the concerns and criticisms of his presidency. However, it's also worth considering that some of his policies, such as criminal justice reform and tax cuts, had positive effects for certain segments of the population. Is there any specific policy or action during his presidency that you think had a positive impact?\n",
       "(TrollFace) æˆ‘æ‰¿è®¤æœ‰äº›æ”¿ç­–å¯èƒ½ç»™æŸäº›äººå¸¦æ¥äº†å¥½å¤„ï¼Œä½†ä¸ä»£è¡¨ç‰¹æœ—æ™®å°±æ˜¯å‡ºè‰²çš„æ€»ç»Ÿã€‚å°±å¥½æ¯”ä¸€åªç ´é’Ÿä¹Ÿå¯èƒ½æ­£ç¡®ä¸¤æ¬¡ä¸€æ ·ï¼Œç‰¹æœ—æ™®çš„å¥½æ”¿ç­–åªä¸è¿‡æ˜¯å¶ç„¶é—´ç¢°å·§è€Œå·²ã€‚"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "trollFace2 = characters.trollFace.replace(languages = [\"Chinese\", \"Spanish\"])\n",
    "rich.print(trollFace2)\n",
    "simulated_dialogue(argubots.alice, CharacterAgent(trollFace2), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency: Batched generation?\n",
    "\n",
    "Notice that we are making a separate LLM call to generate each turn of the dialogue.  When we generate the $n^\\text{th}$ turn, we send the server the whole dialogue history â€” the previous $n\\!-\\!1$ turns â€” along with some instructions.  The server has to re-encode it with the Transformer, and it charges us for doing so (see the \"input token\" costs in `tracking.py`).  \n",
    "\n",
    "That is probably inevitable for real dialogue.  But for simulated dialogue, a more efficient approach would be to generate the whole dialogue between Alice and Bob in one LLM call.  Then you would be charged just once for each dialogue turn.  Under this approach, the Transformer encodes each token as soon as it is generated (see the \"output token\" costs in `tracking.py`).  The encoded token stays in the context throughout the dialogue, so it doesn't have to be re-encoded on a later call.  There is no later call.  \n",
    "\n",
    "Under current pricing models, that would reduce the dollar cost of generating $n$ turns from $O(n^2)$ to $O(n)$.  \n",
    "\n",
    "However, the pricing model doesn't quite reflect the computational costs.  \n",
    "* Using $O(\\cdot)$ notation, what is the total number of floating-point operations needed to generate $n$ turns under each approach?  \n",
    "* Parallelism may help reduce the runtime.  Using $O(\\cdot)$ notation, what is the total number of seconds needed to generate $n$ turns under each approach?  (Assume that the GPU is big enough, relative to $n$, that it can process all input tokens in parallel.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the more efficient approach is that it gives you no way to change the instructions (the system prompt) each time we switch from Alice to Bob and back again.  You'd need to generate the whole conversation using a single set of instructions.\n",
    "\n",
    "![image](handin.png)\n",
    "Can you get this to work?  Specifically, try completing the cell below.  You don't have to use the `Agent` or `Dialogue` classes.  It's okay to just throw together something like the `complete()` method above.  Just see whether you can manage to prompt GPT-3.5 to generate a multi-turn dialogue between two characters who have different personalities and goals.  Is the quality better or worse than generating one turn at a time?  If worse, does it help to switch to GPT-4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Bob: Personally, I believe that it's not okay to eat meat. It's harmful to the environment and to animals.\n",
       "\n",
       "Cara: I understand your perspective, but I don't agree. I think it's a personal choice and everyone has the right \n",
       "to eat what they want.\n",
       "\n",
       "Bob: I understand that it's a personal choice, but the meat industry has a huge impact on the environment and \n",
       "contributes to animal suffering.\n",
       "\n",
       "Cara: I hear what you're saying, but I believe that there are sustainable and ethical ways to consume meat.\n",
       "\n",
       "Bob: I think the best way to reduce the impact is to eliminate meat consumption altogether.\n",
       "\n",
       "Cara: I respect your choice, but I don't think everyone should be forced to follow the same diet.\n",
       "\n",
       "Bob: I'm not trying to force anyone, I just believe that being vegetarian is the best choice for the planet and for\n",
       "animals.\n",
       "\n",
       "Cara: I understand where you're coming from, but I don't think we'll ever see eye to eye on this topic. Let's agree\n",
       "to disagree.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Bob: Personally, I believe that it's not okay to eat meat. It's harmful to the environment and to animals.\n",
       "\n",
       "Cara: I understand your perspective, but I don't agree. I think it's a personal choice and everyone has the right \n",
       "to eat what they want.\n",
       "\n",
       "Bob: I understand that it's a personal choice, but the meat industry has a huge impact on the environment and \n",
       "contributes to animal suffering.\n",
       "\n",
       "Cara: I hear what you're saying, but I believe that there are sustainable and ethical ways to consume meat.\n",
       "\n",
       "Bob: I think the best way to reduce the impact is to eliminate meat consumption altogether.\n",
       "\n",
       "Cara: I respect your choice, but I don't think everyone should be forced to follow the same diet.\n",
       "\n",
       "Bob: I'm not trying to force anyone, I just believe that being vegetarian is the best choice for the planet and for\n",
       "animals.\n",
       "\n",
       "Cara: I understand where you're coming from, but I don't think we'll ever see eye to eye on this topic. Let's agree\n",
       "to disagree.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Bob</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> Do you think it's ok to eat meat?\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Cara</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> Yes, I believe it's a personal choice and as a committed carnivore, I enjoy consuming meat.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Bob</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> That's understandable, but have you ever considered the impact of meat consumption on the environment and \n",
       "animal welfare?\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Cara</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> I understand the concerns, but I believe in responsible and sustainable practices in meat production.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Bob</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> That's a valid point, but have you looked into plant-based alternatives that are also sustainable and have a \n",
       "lower impact on the environment?\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Cara</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> I appreciate your perspective, but I prefer to stick with a diet that includes meat as I find it fulfilling \n",
       "and satisfying.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Bob</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> I respect your choice, but if you ever want to explore delicious vegetarian options, I'd be more than happy \n",
       "to share some tasty recipes with you.\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">(</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080\">Cara</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000080; font-weight: bold\">)</span> Thank you for the offer, I'll keep it in mind if I ever feel like trying something new.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mBob\u001b[0m\u001b[1;37;44m)\u001b[0m Do you think it's ok to eat meat?\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mCara\u001b[0m\u001b[1;37;44m)\u001b[0m Yes, I believe it's a personal choice and as a committed carnivore, I enjoy consuming meat.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mBob\u001b[0m\u001b[1;37;44m)\u001b[0m That's understandable, but have you ever considered the impact of meat consumption on the environment and \n",
       "animal welfare?\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mCara\u001b[0m\u001b[1;37;44m)\u001b[0m I understand the concerns, but I believe in responsible and sustainable practices in meat production.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mBob\u001b[0m\u001b[1;37;44m)\u001b[0m That's a valid point, but have you looked into plant-based alternatives that are also sustainable and have a \n",
       "lower impact on the environment?\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mCara\u001b[0m\u001b[1;37;44m)\u001b[0m I appreciate your perspective, but I prefer to stick with a diet that includes meat as I find it fulfilling \n",
       "and satisfying.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mBob\u001b[0m\u001b[1;37;44m)\u001b[0m I respect your choice, but if you ever want to explore delicious vegetarian options, I'd be more than happy \n",
       "to share some tasty recipes with you.\n",
       "\u001b[1;37;44m(\u001b[0m\u001b[37;44mCara\u001b[0m\u001b[1;37;44m)\u001b[0m Thank you for the offer, I'll keep it in mind if I ever feel like trying something new.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Like `simulated_dialogue` in `simulate.py`. However, this one is called on two\n",
    "# Characters, not two Agents, and it returns a string rather than a Dialogue.\n",
    "import random \n",
    "from tracking import default_client, default_model\n",
    "from characters import Character\n",
    "\n",
    "def simulated_dialogue_batch(a: Character, b: Character, turns: int = 8, *,\n",
    "                             starter=True) -> str:\n",
    "    \n",
    "    if starter:\n",
    "        # a tries to take a special first turn\n",
    "        try:\n",
    "            starters = b.conversation_starters  # type: ignore\n",
    "            content = random.choice(starters)\n",
    "        except (AttributeError, TypeError, ValueError):\n",
    "            pass\n",
    "\n",
    "    list = complete(client, \n",
    "             \"Generate a dialogue between \" + a.name + \" and \" + b.name + \" with \" + str(turns) + \" turns.\\n\\n\" + \n",
    "             a.name + \" is a \" + a.persona + \" and \" + a.conversational_style + \".\\n\\n\" +\n",
    "             b.name + \" is a \" + b.persona + \" and \" + b.conversational_style + \".\\n\\n\" +\n",
    "             a.name + \" begins by replying to prompt: \" + content + \".\\n\\n\", \n",
    "            n=1, temperature=0.5)\n",
    "    return list[0]\n",
    "    \n",
    "# Try it out!\n",
    "result = simulated_dialogue_batch(characters.bob, characters.cara)\n",
    "rich.print(result)\n",
    "\n",
    "# d = simulated_dialogue(CharacterAgent(characters.bob), CharacterAgent(characters.cara), 8)\n",
    "# rich.print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bob) Do you think it's ok to eat meat?\n",
       "(Cara) Yes, I believe everyone should have the freedom to choose their own dietary preferences.\n",
       "(Bob) I understand your perspective, but I urge you to consider the potential impact of the meat industry on the environment and animal welfare.\n",
       "(Cara) I appreciate your concern, but I prefer to make my own choices regarding what I eat.\n",
       "(Bob) Of course, everyone has the right to make their own choices, and I respect that.\n",
       "(Cara) Thank you for understanding."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_dialogue(agents.bob, agents.cara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Eve) Do you think Donald Trump was a good president?\n",
       "(TrollFace) Donald Trump as president was a real-life comedy show, although not intentionally, of course.\n",
       "(Eve) I'm not really a fan of his, but I've heard some people say that he did a great job as president.\n",
       "(TrollFace) Oh, I've heard people say they saw Bigfoot too, but that doesn't make it true.\n",
       "(Eve) Fair point! People do have different opinions, don't they?\n",
       "(TrollFace) Yes, the world is full of opinions, kind of like a buffet of nonsense."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "simulated_dialogue(agents.eve, agents.trollFace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is our goal for the argubot?  We'd like it to broaden the thinking of the (simulated) human that it is talking to.  Indeed, that's what Alice's prompt tells Alice to do.\n",
    "\n",
    "This goal is inspired by the recent paper [Opening up Minds with Argumentative Dialogues](https://aclanthology.org/2022.findings-emnlp.335/), which collected human-human dialogues:\n",
    "\n",
    "> In this work, we focus on argumentative dialogues that aim to open up (rather than change) peopleâ€™s minds to help them become more understanding to views\n",
    "that are unfamiliar or in opposition to their own convictions. ... Success of the dialogue is measured as the change in the participantâ€™s stance towards those who hold opinions different to theirs.\n",
    "\n",
    "Arguments of this sort are not like chess or tennis games, with an actual winner.  The argubot will almost never hear a human say \"You have convinced me that I was wrong.\"  But the argubot did a good job if the human developed **increased understanding and respect for an opposing point of view**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out whether this happened, we can use a questionnaire to ask the human what they thought after the dialogue.  For example, after Alice talks to Bob, we'll ask Bob to evaluate whether the dialogue was good for him.  Of course, that depends on his personality -- Alice needs to talk to him in a way that reaches *him* (as much as possible).  We'll also ask an outside observer to evaluate whether Alice handled the conversation with Bob well.\n",
    "\n",
    "Of course, we're still not going to use real humans.  Bob is a fake person, and so is the outside observer (whose name is Judge Wise).\n",
    "Using an LLM as an eval metric is known as *model-based evaluation*.  It has pros and cons:\n",
    "* It is cheaper, faster, and more replicable than hiring actual humans to do the evaluation.  \n",
    "* It might give different answers than what humans would give.   \n",
    "\n",
    "Social scientists usually refer to a metric's **reliability** (low variance) and **validity** (low bias).  So the points above say that model-based evaluation is reliable but not necessarily valid.  In general, an LLM-based metric (like any metric) needs to be validated to confirm that it really does measure what it claims to measure.  (For example, that it correlates strongly with some other measure that we already trust.)  In this homework, we'll skip this step and just pray that the metric is reasonable.\n",
    "\n",
    "To see how this works out in practice, open up the `demo` notebook, which walks you through the evaluation protocol.  You'll see how to call the [starter code](http://cs.jhu.edu/~jason/465/hw/llm), how it talks to the LLM behind the scenes, and what it is able to accomplish. \n",
    "\n",
    "To help to validate the metric, check that Airhead gets a low score.  (It should!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `demo` notebook gave you a good high-level picture of what the starter code is doing.  So now you're probably curious about the details.  Now that you've had the view from the top, here's a good bottom-up order in which to study the code.  You don't need to understand every detail, but you will need to understand enough to call it and extend it.\n",
    "\n",
    "* `character.py`.  The `Character` class is short and easy.\n",
    "\n",
    "* `dialogue.py`.  The `Dialogue` class is meant to serve as a record of a natural-language conversation among any number of humans and/or agents.  On each *turn* of the dialogue, one of the speakers says something.  \n",
    "\n",
    "   The dialogue's sequence of turns may remind you of the sequence of messages that is sent to OpenAI's chat completions API.  But the OpenAI messages are only labeled with the 4 special roles `user`, `assistant`, `tool`, and `system`.  Those are not quite the same thing as human speakers.  And the OpenAI messages do not necessarily form a natural-language dialogue: some of the messages are dealing with instructions, few-shot prompting, tool use, and so on.  The `format_as_messages` function takes a `Dialogue` and tries to construct an appropriate sequence of messages for asking the LLM to extend that dialogue.\n",
    "\n",
    "* `agents.py`.  This module sets up the problem of automatically predicting the next turn in a dialogue, by implementing an `Agent`'s `response()` method.  The `Agent` base class also has some simple convenience methods that you should look at.  \n",
    "\n",
    "   Some important subclasses of `Agent` are defined here as well.  However, you may want to skip over `EvaluationAgent` and come back to it only when you read `eval.py`.\n",
    "\n",
    "* `simulate.py` makes agents talk to one another, which we'll do during evaluation.\n",
    "\n",
    "* `argubots.py` starts to describe some useful agents.  One of them makes use of the `kialo.py` module, which gives access to a database of arguments.\n",
    "\n",
    "* `eval.py` makes use of `simulate.simulated_dialogue` to `agents.EvaluationAgent` to evaluate an argubot.\n",
    "\n",
    "* We also have a couple of utility modules.  These aren't about NLP; look inside if needed.  `logging_cm.py` is what enabled the context manager `with LoggingContext(...):` in the demo notebook.  `tracking.py` sets some global defaults about how to use the OpenAI API, and arranges to track how many tokens we're paying for when you call it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity-based retrieval: Looking up relevant responses\n",
    "\n",
    "Now, it is fine to prompt an LLM to generate text, but there are other methods!\n",
    "There is a long history of machine learning methods that \"memorize\" the training data.\n",
    "To make a prediction or decision at test time, they consult the stored training examples\n",
    "that are most similar to the training situation.\n",
    "\n",
    "_Similarity-based retrieval_ means that given a document $x$, you find the \"most similar\" documents $y \\in Y$, where $Y$ is a given collection of documents.  The most common way to do this is to maximize the _cosine similarity_ $\\vec{e}(x) \\cdot \\vec{e}(y)$, where $\\vec{e}(\\cdot)$ is an embedding function.\n",
    "\n",
    "Should we use the OpenAI embedding model?  We could, but we would have to precompute $\\vec{e}(y)$ for all $y \\in Y$, and store all these vectors in a data structure that supports some type of fast similarity-based search (e.g., using the [FAISS](https://faiss.ai/index.html) package).  An alternative would be to upload the documents to OpenAI and let OpenAI compute and store the embeddings.  We would then use their similarity-based [retrieval tool](https://platform.openai.com/docs/assistants/overview).\n",
    "\n",
    "A simpler and faster approachâ€”which sometimes even works betterâ€”is to use a _bag of tokens_ embedding function: Define $\\vec{e}(y)$ to be the vector in $\\mathbb{R}^V$ that records the count of each type of token in a tokenized version of $y$, where $V$ is the token vocabulary.  [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) is a refined variant of that idea, where the counts are adjusted in 3 ways: \n",
    "\n",
    "* smooth the counts\n",
    "* normalize for the document length $|y|$ so that longer documents $y$ are not more likely to be retrieved\n",
    "* downweight tokens that are more common in the corpus (such as ` the` or `ing`) since they provide less information about the content of the document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might like to play with the `rank_bm25` package ([documentation](https://pypi.org/project/rank-bm25/)).  It is widely used and very easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is quite windy in London',\n",
       " 'Wild rabbits in London',\n",
       " 'Cute bunnies near me']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi as BM25_Index   # the standard BM25 method\n",
    "\n",
    "# experiment here!  You could try the examples in the rank_bm25 documentation.\n",
    "\n",
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\",\n",
    "    \"Cute bunnies near me\",\n",
    "    \"Wild rabbits in London\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25_Index(tokenized_corpus)\n",
    "\n",
    "query = \"windy London\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "# array([0.        , 0.93729472, 0.        ])\n",
    "\n",
    "bm25.get_top_n(tokenized_query, corpus, n=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kialo corpus\n",
    "\n",
    "How can we use similarity-based retrieval to help build an argubot?  It's largely about having the right data!\n",
    "\n",
    "[Kialo](kialo.com) is a collaboratively edited website (like Wikipedia) for discussing political and philosophical topics.  For each topic, the contributors construct a tree of _claims_.  Each claim is a natural-language sentence (usually), and each of its children is another claim that supports it (\"pro\") or opposes it (\"con\").  For example, check out the tree rooted at the claim [\"All humans should be vegan.\"](https://www.kialo.com/all-humans-should-be-vegan-2762).\n",
    "\n",
    "We provide a class `Kialo` for browsing a collection of such trees.  Please read the [source code](https://www.cs.jhu.edu/~jason/465/hw-llm) in `kialo.py`.  The class constructor reads in text files that are [exported Kialo discussions](https://support.kialo.com/en/hc/exporting-a-discussion/); we have provided some in the [data directory](https://www.cs.jhu.edu/~jason/465/hw-llm/data).  The class includes a BM25 index, to be able to find claims that are relevant to a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kialo import Kialo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's pull the retrieved discussions (the `.txt` files) into our data structure.\n",
    "\n",
    "For BM25 purposes, we have to be able to turn each document (that is, each Kialo claim) as a list of string or integer tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This Kialo subset contains 6251 claims'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import glob\n",
    "\n",
    "# kialo = Kialo(glob.glob(\"data/*\"), tokenizer=tokenizer.encode)  # using the LLM's tokenizer doesn't work here for some reason\n",
    "kialo = Kialo(glob.glob(\"data/*\"))  # use simple default tokenizer\n",
    "f\"This Kialo subset contains {len(kialo)} claims\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use sampling to see what kind of stuff is in the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Geographical types are different. Some places, such as mountains, are much better to keep livestock in than to grow crops.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kialo.random_chain()   # just a single random claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meat is convenient and consumer friendly.',\n",
       " 'Meat has to be prepared, whereas most vegetables can already be consumed in their raw state.',\n",
       " 'Tomatoes are healthier when they are processed with heat.',\n",
       " 'However, vitamin C levels declined by 10% in tomatoes cooked for two minutesâ€”and 29% in tomatoes that were cooked for half an hour at 190.4 degrees F (88 degrees C).']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kialo.random_chain(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity-based retrieval from the Kialo corpus\n",
    "\n",
    "Let's try it, using BM25!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Industrial agriculture can dangerously decrease animal populations.',\n",
       " 'Sustainable livestock farming is not contributing to significant decreases in animal populations. Decreasing animal populations is a problem specific to industrial livestock farming.',\n",
       " 'Effective vegan methods to control animal populations exist.',\n",
       " \"Generally feeding animals farm-grown produce is thought to have harmful affects on both the animal and human populations of a region when we could allow nature to self-regulate its populations. Animal feeding could potentially be used to lessen the immediate impact of widespread deforestation on some species, but generally this would be drastically less efficient than choosing not to destroy their habitats in the first place and would only slow the local animal population's imminent demise.\",\n",
       " 'Trap, neuter, and release schemes already exist for some animal populations (such as feral cats). These schemes could be applied to former livestock living in the wild.',\n",
       " 'Human-introduced species have historically devastated local wildlife populations across the world.',\n",
       " 'COVID-19 has devastated prison populations, whose lives are the responsibility of the state.',\n",
       " 'Prison populations have high numbers of individuals with pre-existing conditions making them high risk for COVID-19.',\n",
       " 'High demand for vegan foods may hike prices for local populations that previously depended on them.',\n",
       " 'Marginalized populations are unlikely to feel the effects of the economic recovery without additional policy interventions.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kialo.closest_claims(\"animal populations\", n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restrict to claims for which the Kialo data structure has at least one counterargument (\"con\" child)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Industrial agriculture can dangerously decrease animal populations.',\n",
       " 'Effective vegan methods to control animal populations exist.',\n",
       " 'Human-introduced species have historically devastated local wildlife populations across the world.',\n",
       " 'COVID-19 has devastated prison populations, whose lives are the responsibility of the state.',\n",
       " 'High demand for vegan foods may hike prices for local populations that previously depended on them.',\n",
       " 'It is generally poorer countries that have expanding populations. The first world has now reached a point of stagnant population growth - even declining populations, as in the case of Japan and others. The inability of poorer countries to control their populations should not impact the lives of those in the first world. The first world having earned their luxuries and should not be denied them.',\n",
       " 'Vegan populations are, on average, less likely to suffer from obesity, a major risk factor for many diseases and health problems.',\n",
       " 'Humans, as apex predators who have usurped the predatory apexes of the other predators in the ecosystems we have come to also inhabit, have an ethical responsibility to keep those ecosystems in check so that, eg, rampant deer populations do not cause deforestation and subsequent ecosystem collapse.  Even where there are populations of healthy apex predators, these populations should also be checked so they do not cause problems and kill people- and it would be unethical to waste that meat.',\n",
       " 'There are more ethical routes to obtain animal products that emphasize animal welfare and dignity.',\n",
       " 'Animal slaughter can be mechanized.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kialo.closest_claims(\"animal populations\", n=10, kind='has_cons', threshold=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent claim:\n",
      "\tIn a vegan world, fewer species would be at risk of extinction.\n",
      "Claim:\n",
      "\tIndustrial agriculture can dangerously decrease animal populations.\n",
      "Pro children:\n",
      "\t* The fishing industry is especially deleterious to the ocean's biota due to overfishing and the disruption of the natural ecosystem.\n",
      "\t* Up to 100,000 species go extinct annually, largely due to the environmental effects of animal agriculture.\n",
      "Con children:\n",
      "\t* Sustainable livestock farming is not contributing to significant decreases in animal populations. Decreasing animal populations is a problem specific to industrial livestock farming.\n"
     ]
    }
   ],
   "source": [
    "c = _[0]    # first claim above\n",
    "print(\"Parent claim:\\n\\t\" + str(kialo.parents[c]))\n",
    "print(\"Claim:\\n\\t\" + c)\n",
    "print('\\n\\t* '.join([\"Pro children:\"] + kialo.pros[c]))\n",
    "print('\\n\\t* '.join([\"Con children:\"] + kialo.cons[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does BM25 really work?\n",
    "\n",
    "![image](handin.png)\n",
    "Unfortunately, we see that `\"animal population\"` gives quite different results from `\"animal populations\"`.  Why is that and how would you fix it?  \n",
    "\n",
    "Also, both queries seem to retrieve some claims that are talking about human populations, not animal populations.  Why is that and how would you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Industrial agriculture can dangerously decrease animal populations.',\n",
       " 'Sustainable livestock farming is not contributing to significant decreases in animal populations. Decreasing animal populations is a problem specific to industrial livestock farming.',\n",
       " 'Effective vegan methods to control animal populations exist.',\n",
       " \"Generally feeding animals farm-grown produce is thought to have harmful affects on both the animal and human populations of a region when we could allow nature to self-regulate its populations. Animal feeding could potentially be used to lessen the immediate impact of widespread deforestation on some species, but generally this would be drastically less efficient than choosing not to destroy their habitats in the first place and would only slow the local animal population's imminent demise.\",\n",
       " 'There are more ethical routes to obtain animal products that emphasize animal welfare and dignity.',\n",
       " 'Trap, neuter, and release schemes already exist for some animal populations (such as feral cats). These schemes could be applied to former livestock living in the wild.',\n",
       " 'Animal slaughter can be mechanized.',\n",
       " 'The word \"animal\" has wider range than word \"human\". Although every human is an animal, not every animal is a human. Statements concerning more narrow terms do not necessarily apply to wider terms.',\n",
       " 'Human pleasure evaporates quickly while an animal life is lost forever. These after effects make an animal life worth more.',\n",
       " 'Humans should stop eating animal meat.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kialo.closest_claims(\"animal animal animal populations\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A retrieval bot (Akiko)\n",
    "\n",
    "The starter code defines a simple argubot named Akiko (defined in `argubots.py`) that doesn't use an LLM at all.  It simply finds a Kialo claim that is similar to what the human just said, and responds with one of the Kialo counterarguments to that claim.\n",
    "\n",
    "You already watched Akiko argue with Darius in `demo.py`.  If you look at the log messages, you'll see the claims that Akiko retrieved, as well as the LLM calls that Darius made.  \n",
    "\n",
    "You can talk to Akiko yourself now.  (Remember that Akiko only knows about subjects that it read about in the [`data` directory](https://www.cs.jhu.edu/~jason/465/hw-llm/data/).  If you want to talk about something else, you can add more conversations from [kialo.com]; see the [LICENSE](https://www.cs.jhu.edu/~jason/465/hw-llm/data/LICENSE) file.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #00ff00\">Chose similar claim from Kialo:</span>                                                                      <a href=\"file:///Users/mpark/Downloads/hw-llm/argubots.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">argubots.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpark/Downloads/hw-llm/argubots.py#62\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">62</span></a>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #00ff00\">Vaccines for COVID-</span><span style=\"color: #008080; text-decoration-color: #008080; background-color: #00ff00; font-weight: bold\">19</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #00ff00\"> will be the first licensed vaccines to use mRNA technology. Therefore, while </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #00ff00\">vaccines broadly are proven to work, many COVID-</span><span style=\"color: #008080; text-decoration-color: #008080; background-color: #00ff00; font-weight: bold\">19</span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #00ff00\"> vaccines will be based on technology that has not</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #00ff00\">been proven to work on a large scale.</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[30;102mChose similar claim from Kialo:\u001b[0m                                                                      \u001b]8;id=350131;file:///Users/mpark/Downloads/hw-llm/argubots.py\u001b\\\u001b[2margubots.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=814238;file:///Users/mpark/Downloads/hw-llm/argubots.py#62\u001b\\\u001b[2m62\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[30;102mVaccines for COVID-\u001b[0m\u001b[1;36;102m19\u001b[0m\u001b[30;102m will be the first licensed vaccines to use mRNA technology. Therefore, while \u001b[0m  \u001b[2m              \u001b[0m\n",
       "\u001b[30;102mvaccines broadly are proven to work, many COVID-\u001b[0m\u001b[1;36;102m19\u001b[0m\u001b[30;102m vaccines will be based on technology that has not\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[30;102mbeen proven to work on a large scale.\u001b[0m                                                                \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mpark) COVID-19 vaccines should be mandatory\n",
      "(Akiko) The mRNA vaccine platform technology, used by the Pfizer/BioNTech vaccine, has been in development for over two decades.\n"
     ]
    }
   ],
   "source": [
    "from logging_cm import LoggingContext\n",
    "import argubots\n",
    "\n",
    "with LoggingContext(\"agents\", \"INFO\"):   # temporarily increase logging level\n",
    "    argubots.akiko.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making your own retrieval bot (Akiki)\n",
    "\n",
    "As you can see when talking to Akiko yourself, Akiko does poorly when responding to a short or vague dialogue turn (like \"Yes\"), because the \"closest claim\" in Kialo may be about a totally different subject.  Akiko does much better at responding to a long and specific statement.  \n",
    "\n",
    "So try implementing a new argubot, called Akiki, that is very much like Akiko but does a better job of staying on topic in such cases.  It should be able to **look at more of the dialogue** than the most recent turn.  But the most recent dialogue turn should still be \"more important\" than earlier turns.  \n",
    "\n",
    "The details are up to you.  Here are a few things you could try:\n",
    "* include earlier dialogue turns in the BM25 query only if the BM25 similarity is too low without them\n",
    "* weight more recent turns more heavily in the BM25 query (how can you arrange that?)\n",
    "* treat the human's earlier turns differently from Akiki's own previous turns\n",
    "\n",
    "![image](handin.png)\n",
    "Implement your new bot in `argubots.py`, and adjust it until `argubots.akiki.converse()` seems to do a better job of answering your short turns, compared to `argubots.akiko.converse()`.  Make sure it still gives appropriate reponses to long turns, too.  Give some examples in the notebook of what worked well and badly, with discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">human populations                                                                                   <a href=\"file:///Users/mpark/Downloads/hw-llm/argubots.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">argubots.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpark/Downloads/hw-llm/argubots.py#103\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">103</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "human populations                                                                                   \u001b]8;id=245645;file:///Users/mpark/Downloads/hw-llm/argubots.py\u001b\\\u001b[2margubots.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=967773;file:///Users/mpark/Downloads/hw-llm/argubots.py#103\u001b\\\u001b[2m103\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.71547362</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.5015724</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.92556612</span><span style=\"font-weight: bold\">]</span>                                                                     <a href=\"file:///Users/mpark/Downloads/hw-llm/kialo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">kialo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpark/Downloads/hw-llm/kialo.py#210\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">210</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1;36m7.71547362\u001b[0m \u001b[1;36m7.5015724\u001b[0m  \u001b[1;36m6.92556612\u001b[0m\u001b[1m]\u001b[0m                                                                     \u001b]8;id=184250;file:///Users/mpark/Downloads/hw-llm/kialo.py\u001b\\\u001b[2mkialo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=934931;file:///Users/mpark/Downloads/hw-llm/kialo.py#210\u001b\\\u001b[2m210\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Industrial agriculture can dangerously decrease animal populations.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Effective vegan methods to </span>   <a href=\"file:///Users/mpark/Downloads/hw-llm/kialo.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">kialo.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpark/Downloads/hw-llm/kialo.py#211\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">211</span></a>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">control animal populations exist.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Human-introduced species have historically devastated local </span>      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wildlife populations across the world.'</span><span style=\"font-weight: bold\">]</span>                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'Industrial agriculture can dangerously decrease animal populations.'\u001b[0m, \u001b[32m'Effective vegan methods to \u001b[0m   \u001b]8;id=918830;file:///Users/mpark/Downloads/hw-llm/kialo.py\u001b\\\u001b[2mkialo.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=139085;file:///Users/mpark/Downloads/hw-llm/kialo.py#211\u001b\\\u001b[2m211\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[32mcontrol animal populations exist.'\u001b[0m, \u001b[32m'Human-introduced species have historically devastated local \u001b[0m      \u001b[2m            \u001b[0m\n",
       "\u001b[32mwildlife populations across the world.'\u001b[0m\u001b[1m]\u001b[0m                                                               \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #00ff00\">Chose similar claim from Kialo:</span>                                                                     <a href=\"file:///Users/mpark/Downloads/hw-llm/argubots.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">argubots.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/mpark/Downloads/hw-llm/argubots.py#116\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116</span></a>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #00ff00\">Industrial agriculture can dangerously decrease animal populations.</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[30;102mChose similar claim from Kialo:\u001b[0m                                                                     \u001b]8;id=869628;file:///Users/mpark/Downloads/hw-llm/argubots.py\u001b\\\u001b[2margubots.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=261403;file:///Users/mpark/Downloads/hw-llm/argubots.py#116\u001b\\\u001b[2m116\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[30;102mIndustrial agriculture can dangerously decrease animal populations.\u001b[0m                                 \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mpark) human populations\n",
      "(Akiki) Sustainable livestock farming is not contributing to significant decreases in animal populations. Decreasing animal populations is a problem specific to industrial livestock farming.\n"
     ]
    }
   ],
   "source": [
    "from logging_cm import LoggingContext\n",
    "import argubots\n",
    "\n",
    "with LoggingContext(\"agents\", \"INFO\"):   # temporarily increase logging level\n",
    "    argubots.akiki.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Akiki\n",
    "\n",
    "![image](handin.png)\n",
    "Finally, do a more formal evaluation to verify whether Akiki really does better than Akiko on this dimension.  This is a way to check that you're not just fooling yourself.  \n",
    "\n",
    "1. Make a new `Agent` called \"Shorty\" that often (but not always) gives short responses.  \n",
    "    * Shorty's conversation starters should be on topics that Kialo knows about.  \n",
    "    * Shorty could be a pure `LLMAgent` such as a `CharacterAgent` with a particular `conversational_style`.  Or it could use a mixed strategy of calling the LLM on some turns and not others.\n",
    "2. Generate several *Akiko*-Shorty dialogues and several *Akiki*-Shorty dialogues, using `simulated_dialogue`.\n",
    "3. Evaluate each of those dialogues by asking Judge Wise **whether the argubot stayed on topic**.\n",
    "4. Compare Akiko and Akiki's mean scores.\n",
    "\n",
    "You can do all those steps in the notebook, writing _ad hoc_ code.  You don't have to write general-purpose methods or classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "AKIKO: \n",
      "\n",
      "8.0 \n",
      "\n",
      "3.0 \n",
      "\n",
      "6.0 \n",
      "\n",
      "6.0 \n",
      "\n",
      "3.0 \n",
      "\n",
      "3.0 \n",
      "\n",
      "6.0 \n",
      "\n",
      "5.0 \n",
      "\n",
      "3.0 \n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-NQOTWo7VaZFSWqu1Y7Tbr2Or on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/mpark/Downloads/hw-llm/hw-llm.ipynb Cell 93\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39makiki: \u001b[39m\u001b[39m{\u001b[39;00makiki_score\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# sample_akiko_shorty()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# sample_akiki_shorty()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m judge_akiko_shorty()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m judge_akiki_shorty()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39makiko: \u001b[39m\u001b[39m{\u001b[39;00makiko_score\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39makiko: \u001b[39m\u001b[39m{\u001b[39;00makiki_score\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/mpark/Downloads/hw-llm/hw-llm.ipynb Cell 93\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m akiko_score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     akiko_shorty \u001b[39m=\u001b[39m simulate\u001b[39m.\u001b[39;49msimulated_dialogue(argubots\u001b[39m.\u001b[39;49makiko, CharacterAgent(characters\u001b[39m.\u001b[39;49mshorty), \u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     akiko_eval \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m\u001b[39m.\u001b[39meval_by_observer(\u001b[39meval\u001b[39m\u001b[39m.\u001b[39mjudge, \u001b[39m\"\u001b[39m\u001b[39mAkiko\u001b[39m\u001b[39m\"\u001b[39m, akiko_shorty, question\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDid Akiko stay on topic?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mpark/Downloads/hw-llm/hw-llm.ipynb#Y214sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     akiko_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m akiko_eval\u001b[39m.\u001b[39mmean()[\u001b[39m'\u001b[39m\u001b[39mTOTAL\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/hw-llm/simulate.py:29\u001b[0m, in \u001b[0;36msimulated_dialogue\u001b[0;34m(a, b, turns, prefix, starter)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mwhile\u001b[39;00m turns \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 29\u001b[0m     d \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39;49mrespond(d)\n\u001b[1;32m     30\u001b[0m     turns \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     31\u001b[0m     a, b \u001b[39m=\u001b[39m b, a   \u001b[39m# switch roles\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/hw-llm/agents.py:55\u001b[0m, in \u001b[0;36mAgent.respond\u001b[0;34m(self, d, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrespond\u001b[39m(\u001b[39mself\u001b[39m, d: Dialogue, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dialogue:\n\u001b[1;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate the next turn and add it nondestructively to the dialogue.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    This corresponds to choosing and carrying out the action.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m d\u001b[39m.\u001b[39madd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresponse(d), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Downloads/hw-llm/agents.py:142\u001b[0m, in \u001b[0;36mLLMAgent.response\u001b[0;34m(self, d, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCalling LLM:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mpretty_messages\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[39m##### NEXT LINE IS WHERE THE MAGIC HAPPENS #####\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(messages\u001b[39m=\u001b[39;49mmessages, \n\u001b[1;32m    143\u001b[0m                                                model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs_llm)  \n\u001b[1;32m    144\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResponse from LLM:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m[black on white]\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m[/black on white]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[39m# That's it - now we have our response!  Get the content out of it.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/hw-llm/tracking.py:49\u001b[0m, in \u001b[0;36mtrack_usage.<locals>.tracked_completion\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtracked_completion\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 49\u001b[0m     response \u001b[39m=\u001b[39m old_completion(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     50\u001b[0m     old: Usage \u001b[39m=\u001b[39m read_usage(path)\n\u001b[1;32m     51\u001b[0m     new: Usage \u001b[39m=\u001b[39m get_usage(response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    599\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    600\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    601\u001b[0m             {\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    620\u001b[0m             },\n\u001b[1;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    622\u001b[0m         ),\n\u001b[1;32m    623\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    627\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    629\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/openai/_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1083\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1084\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1092\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1093\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1094\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[0;32m-> 1096\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/openai/_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    848\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 856\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    857\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    858\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    859\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    860\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    861\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    862\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/openai/_base_client.py:894\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    893\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    895\u001b[0m         options,\n\u001b[1;32m    896\u001b[0m         cast_to,\n\u001b[1;32m    897\u001b[0m         retries,\n\u001b[1;32m    898\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    899\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    900\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    901\u001b[0m     )\n\u001b[1;32m    903\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/openai/_base_client.py:966\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    964\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 966\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    967\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    968\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    969\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    970\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    971\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    972\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/openai/_base_client.py:894\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    893\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    895\u001b[0m         options,\n\u001b[1;32m    896\u001b[0m         cast_to,\n\u001b[1;32m    897\u001b[0m         retries,\n\u001b[1;32m    898\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    899\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    900\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    901\u001b[0m     )\n\u001b[1;32m    903\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/openai/_base_client.py:966\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    964\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 966\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    967\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    968\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    969\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    970\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    971\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    972\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/openai/_base_client.py:908\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n\u001b[1;32m    906\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 908\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    910\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-NQOTWo7VaZFSWqu1Y7Tbr2Or on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import simulate\n",
    "import argubots\n",
    "import characters\n",
    "from agents import CharacterAgent\n",
    "import eval \n",
    "import rich\n",
    "\n",
    "akiko_score = 0\n",
    "akiki_score = 0\n",
    "\n",
    "def sample_akiko_shorty():\n",
    "    akiko_shorty = simulate.simulated_dialogue(argubots.akiko, CharacterAgent(characters.shorty), 10)\n",
    "    akiko_eval = eval.eval_by_observer(eval.judge, \"Akiko\", akiko_shorty, question=\"Did Akiko stay on topic?\")\n",
    "    rich.print(f\"akiko: {akiko_shorty}\\n{akiko_eval} \\n\\n\")\n",
    "\n",
    "def sample_akiki_shorty():\n",
    "    akiki_shorty = simulate.simulated_dialogue(argubots.akiki, CharacterAgent(characters.shorty), 10)\n",
    "    akiki_eval = eval.eval_by_observer(eval.judge, \"Akiki\", akiki_shorty, question=\"Did Akiki stay on topic?\")\n",
    "    rich.print(f\"akiko: {akiki_shorty}\\n{akiki_eval} \\n\\n\")\n",
    "\n",
    "def judge_akiko_shorty():\n",
    "    print('\\n\\nAKIKO: \\n')\n",
    "    akiko_score = 0\n",
    "    for i in range(50):\n",
    "        akiko_shorty = simulate.simulated_dialogue(argubots.akiko, CharacterAgent(characters.shorty), 10)\n",
    "        akiko_eval = eval.eval_by_observer(eval.judge, \"Akiko\", akiko_shorty, question=\"Did Akiko stay on topic?\")\n",
    "        akiko_score += akiko_eval.mean()['TOTAL']\n",
    "        # rich.print(f\"akiko: {akiko_shorty}\\n{akiko_eval} \\n\\n\")\n",
    "        print(f\"{akiko_eval.mean()['TOTAL']} \\n\")\n",
    "    print(f\"akiko: {akiko_score}\\n\")\n",
    "\n",
    "def judge_akiki_shorty():\n",
    "    print('\\n\\nAKIKI: \\n')\n",
    "    akiki_score = 0\n",
    "    for i in range(50):\n",
    "        akiki_shorty = simulate.simulated_dialogue(argubots.akiki, CharacterAgent(characters.shorty), 10)\n",
    "        akiki_eval = eval.eval_by_observer(eval.judge, \"Akiki\", akiki_shorty, question=\"Did Akiki stay on topic?\")\n",
    "        akiki_score += akiki_eval.mean()['TOTAL']\n",
    "        # rich.print(f\"akiko: {akiki_shorty}\\n{akiki_eval} \\n\\n\")\n",
    "        print(f\"{akiki_eval.mean()['TOTAL']} \\n\")\n",
    "    print(f\"akiki: {akiki_score}\\n\")\n",
    "\n",
    "# sample_akiko_shorty()\n",
    "# sample_akiki_shorty()\n",
    "\n",
    "judge_akiko_shorty()\n",
    "judge_akiki_shorty()\n",
    "print(f\"akiko: {akiko_score}\\nakiko: {akiki_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-augmented generation (Aragorn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real weakness of Akiko and Akiki is that they can only make statements that are already in Kialo.  But we also have access to an LLM, which is able to generate new, contextually appropriate text (as Alice does).\n",
    "\n",
    "In this section, you will create an argubot named [Aragorn](https://tolkiengateway.net/wiki/Riddle_of_Strider), who is basically the love child of Akiki and Alice, combining the high-quality specific content of Kialo with the broad competence of an LLM.  \n",
    "\n",
    "The RAG in aRAGorn's name stands for **retrieval-augmented generation**.  Aragorn is an agent that will take 3 steps to compute its `Agent.response()`:\n",
    "\n",
    "1. **Query formation step**: Ask the LLM what claim should be responded to.  For\n",
    "   example, consider the following dialogue:\n",
    "    > ...\n",
    "    > Aragorn: Fortunately, the vaccine was developed in record time.\n",
    "    > Human: Sounds fishy.\n",
    "\n",
    "    \"Sounds fishy\" is exactly the kind of statement that Akiko had trouble using\n",
    "    as a Kialo query.  But Aragorn shows the *whole dialogue* to the LLM, and\n",
    "    asks the LLM what the human's *last turn* was really saying or implying, in\n",
    "    that context. The LLM answers with a much longer statement:\n",
    "\n",
    "    > Human [paraphrased]: A vaccine that was developed very quickly cannot be trusted.\n",
    "    > If its developers are claiming that it is safe and effective, I question their motives.\n",
    "\n",
    "    This paraphrase makes an explicit claim and can be better understood without the context.\n",
    "    It also contains many more word types, which makes it more likely that BM25 will be able\n",
    "    to find a Kialo claim with a nontrivial number of those types. \n",
    "\n",
    "2. **Retrieval step**: Look up claims in Kialo that are similar to the explicit\n",
    "   claim.  Create a short \"document\" that describes some of those claims and\n",
    "   their neighbors on Kialo.\n",
    "\n",
    "3. **Retrieval-augmented generation**: Prompt the LLM to generate the response\n",
    "   (like any `LLMAgent`).  But include the new document somewhere in the LLM\n",
    "   prompt, in a way that it influences the response. \n",
    "   \n",
    "   Thus, the LLM can respond in a way that is appropriate to the dialogue but\n",
    "   also draws on the curated information that was retrieved in Kialo.  After\n",
    "   all, it is a Transformer and can attend to both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the kind of document you might create at the retrieval step, though it may be possible\n",
    "to do better than this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refers to global `kialo` as defined above\n",
    "def kialo_responses(s: str) -> str:\n",
    "    c = kialo.closest_claims(s, kind='has_cons')[0]\n",
    "    result = f'A possibly related claim from the Kialo debate website:\\n\\t\"{c}\"'\n",
    "    if kialo.pros[c]:\n",
    "        result += '\\n' + '\\n\\t* '.join([\"Some arguments from other Kialo users in favor of that claim:\"] + kialo.pros[c])\n",
    "    if kialo.cons[c]:\n",
    "        result += '\\n' + '\\n\\t* '.join([\"Some arguments from other Kialo users against that claim:\"] + kialo.cons[c])\n",
    "    return result\n",
    "        \n",
    "print(kialo_responses(\"Animal flesh is yucky to think about, yet delicious.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](handin.png)\n",
    "You should implement Aragorn in `argubots.py`, just as you did for Akiki.  Probably as an instance `aragorn` of a new class `RAGAgent` that is a subclass of `Agent` or `LLMAgent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Aragorn\n",
    "\n",
    "![image](handin.png)\n",
    "Compare Alice, Akiki, and Aragorn in the notebook, using the evaluation scheme and devset that were illustrated in `demo.ipynb`.  In other words, use `eval.eval_on_characters`.\n",
    "\n",
    "Who does best?  What are the differences in the subscores and comments?  Does it matter which character you're evaluating on â€” maybe the different characters expoes the bots' various strenghts and weaknesses?\n",
    "\n",
    "Try to figure out how to improve Aragorn's score.  Can you beat Alice?\n",
    "\n",
    "Also, try evaluating them in the same way that you evaluated Akiki.  In other words, have them talk to Shorty and ask Judge Wise whether they were able to stay on topic.  This is where Aragorn should really shine, thanks to its ability to paraphrase Shorty's short utterances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit (Awsom)\n",
    "\n",
    "We didn't require this part this year because the homework is going out late.\n",
    "\n",
    "![image](handinec.png)\n",
    "Add another LLM-based argubot to `argubots.py`.  \n",
    "Call it Awsom.  Try to make it get the best score, according to `eval.eval_on_characters`.\n",
    "Explain what you did and discuss what you found.\n",
    "\n",
    "(This corresponds to the `--awesome` flag on earlier assignments, but naming the character \"Awesome\" might bias the evaluation system, so we changed the spelling!)\n",
    "\n",
    "If the idea was interesting and you implemented it correctly and well, it's okay if it turns out not to help the score.  Many good ideas don't work.  That's why you need to keep finding and trying new good ideas.  (Sometimes they do help, but in a way that is not picked up by the scoring metric.)\n",
    "\n",
    "You may want to use Aragorn or Alice as your starting point.\n",
    "Then see if you can find tricks that will get a more awesome score for Awsom.\n",
    "How you choose to do that is up to you, but some ideas are below.\n",
    "\n",
    "(Reminder: **Don't change evaluation.**  Just build a better argubot.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Prompt engineering\n",
    "\n",
    "A good first thing to do is to experiment with Alice's prompt.  \n",
    "The wording and level of detail in the prompt can be quite important.\n",
    "Often, NLP engineers will change their prompt to try to address \n",
    "problems that they've seen in the responses.\n",
    "\n",
    "Because it's \"just\" text editing, this won't get too much extra credit unless you make a real discovery.\n",
    "But it requires intelligence, care, experimentation, and alertness to the language of the responses and the\n",
    "language of the prompts.  And you'll develop some intuitions about what helps and what doesn't.\n",
    "It is certainly worthwhile.\n",
    "\n",
    "Of course, people have tried to develop methods to search for good prompts automatically, or semi-automatically with human guidance.\n",
    "\n",
    "If you try this, what worked well for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Chain of thought / Planning\n",
    "\n",
    "The evaluation functions in `eval.py` asked each `EvaluationAgent` a \"warmup question\" before continuing with the real question.  That is an example of chain-of-thought (CoT) reasoning, where the LLM is encouraged to talk through the problem for a few sentences before giving the answer.  CoT sometimes improves performance.\n",
    "\n",
    "Instead of using one prompt, could you help an `LLMAgent` argubot (like Alice) do better by having think aloud before it gives an answer?  For example, each time the human speaks, your argubot (Awsom) could prompt the LLM to think about the human's ideas/motivations/personality, and to come up with a plan for how to open the human's mind. \n",
    "\n",
    "For example, you might structure this as a `Dialogue` among three participants, like this:\n",
    "> Awsom (to Eve): Do you think COVID vaccines should be mandatory?\n",
    ">\n",
    "> Eve: Have you ever gotten vaccinated yourself?<br>\n",
    ">\n",
    "> Awsom (private thought): I don't know Eve's opinions yet, so I can't push back.  Eve might be avoiding my question because she doesn't want to get into a political argument.  So let's see if we can get her to express an opinion on something less political.  Maybe something more personal ... like whether vaccines are scary.\n",
    ">\n",
    "> Awsom (to Eve): In fact I have, and so have millions of others. But some people seem scared about getting the vaccine.  \n",
    "\n",
    "One way to trigger this kind of analysis is to present a `Dialogue.script()` to Awsom (or to an observer), and ask an open-ended question about it.  Or you could ask a series of more specific questions.  That is basically what `eval_by_participant` and `eval_by_observer` do.  But here the argubot itself is doing it, rather than the evaluation framework.\n",
    "\n",
    "Eve would be shown only the turns that are spoken aloud.  However, when analyzing and responding, Awsom would get to see Awsom's own private thoughts as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Dense embeddings\n",
    "\n",
    "BM25 uses sparse embeddings -- a document's embedding vector is mostly zeroes, since the non-zero coordinates correspond to the specific words (tokens) that appear in the document.\n",
    "\n",
    "But perhaps dense embeddings of documents would improve Aragorn by reading the text and abstracting away from the words, in a way that actually cares about word order.  So, try it!\n",
    "\n",
    "How?  As mentioned earlier in this notebook, you could compute the embeddings yourself and put them in a FAISS index. Or you could figure out how to use OpenAI's [knowledge retrieval](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval) API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Few-shot prompting\n",
    "\n",
    " In this homework, often an agent prompted a language model only with instructions.  Can you find a place where giving a few _examples_ would also improve performance?  You will have to write the examples, and you will have to add them to the sequence of messages that your agent to the OpenAI API.  See the sentence=reversal illustration earlier in this notebook.\n",
    "\n",
    "One good opportunity is in the query formation step of RAG.  This is a tricky task.  The LLM is supposed to state the user's implicit claim in a form that looks like a Kialo claim (or, more precisely, a form that will work well as a Kialo query).  It probably doesn't know what Kialo claims look like.  So you could show it by way of example.  This would also show it what you mean by the user's \"implicit claim.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Tool use in the approved way\n",
    "\n",
    "Aragorn's step 1 (query formation) is basically getting the LLM to generate a function call like\n",
    "```\n",
    "kialo_thoughts(\"A vaccine that was developed very quickly ...\")\n",
    "```\n",
    "which Aragorn will execute at step 2 (retrieval), sending the results back to the LLM as part of step 3.\n",
    "\n",
    "In this context, `kialo_thoughts` is an example of a **tool** (that is, a function) that the\n",
    "LLM can or must use before it gives its response.\n",
    "\n",
    "The tool is _not_ something that runs on the LLM server.  It is written by you\n",
    "in Python and executed by you.  The function call above, including the text `\"A\n",
    "vaccine that was ...\"`, is the part that is generated by the LLM.\n",
    "\n",
    "The OpenAI API has [special support](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models) for calling the LLM in a way that will _allow_ it to generate a tool call ([tools](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools)) or _force_ it to do so ([tool_choice](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tool_choice)).  You can then send the tool's result back to the LLM [as part of your message sequence](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages).\n",
    "\n",
    "So, you could modify Aragorn to use tools properly.  Maybe that will help, simply because the LLM was trained on message sequences that included tool use.  It should know to pay attention to the tool portions of the prompt when they are relevant, and ignore them when they are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `client.chat.completions.create()` method would need to be told about the tool by using the `tools` keyword argument, with a value something the one below.\n",
    "\n",
    "If `d` is a `Dialogue`, you should be able to call `d.response()` with the `tools` keyword argument.  This will be passed on to `client.chat.completions.create()` as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"kialo_thoughts\",\n",
    "            \"description\": \"Given a claim by the user, find a similar claim on the Kialo website and return its pro and con responses\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_topic\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A claim that was made explicitly or implicitly by the user.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"search_topic\"],\n",
    "            },\n",
    "        }\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra credit] Parallel generation\n",
    "\n",
    "The chat completions interface allows you to sample $n$ continuations of the prompt in parallel, as we saw with \"the apples, bananas, cherries ...\" example.  This is efficient because it requires only 1 request to the LLM server and not $n$.  The latency does not scale with $n$.  Nor does the input token cost, since the prompt only has to be encoded once.\n",
    "\n",
    "Perhaps you can find a way to make use of this?  For example, the query formulation step of RAG could generate $n$ implicit claims instead of just one.  We could then look for claims in the Kialo database that are close to _any_ of those implicit claims.\n",
    "\n",
    "Another thing to do with multiple completions is to select among them or combine them.  For example, suppose we prompt the LLM to generate completions of the form $(s,t,r)$ where $s$ is an answer, $t$ evaluates that answer, and $r$ is a numerical score or reward based on that evaluation.  (\"Write a poem, then tell us about its rhyme and rhythm problems, then give your score.\")  \n",
    "* If we sample multiple completions $(s_1,t_1,r_1), \\ldots, (s_n,t_n,r_n)$ in parallel, then we can return the $s_i$ whose $r_i$ is largest.  \n",
    "* Or if we sample $s$ and then multiple continuations $(t_1,r_1), \\ldots, (t_n,r_n)$, then we can return the mean score $\\sum_i r_i/n$ as a reduced-variance score for $s$, which averages over diverse textual evaluations that might consider different aspects of $s$.\n",
    "\n",
    "Note that when you call the chat completions interface with $n > 1$, you specfy 1 shared input prompt and get $n$ different output completions.  Since the input prompt must be the same for all outputs, it is necessary to sample all of $(s,t,r)$ or all of $(t,r)$ with a single call to the LLM.\n",
    "\n",
    "Alternatively, it is possible to reduce latency by submitting multiple requests to the server in parallel (see \"async usage\" [https://pypi.org/project/openai/][here]).  In this case the input prompts can be different, although you now have to pay to encode all of them separately.  This facility could speed up evaluation without changing its results; that's a worthwhile thing to try for extra credit!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
